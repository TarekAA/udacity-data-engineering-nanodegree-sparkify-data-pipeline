# udacity-data-engineering-nanodegree-sparkify-data-pipeline

This project is part of udacity data engineering nano degree program.
see [data-engineer nanodegree page](https://www.udacity.com/course/data-engineer-nanodegree--nd027)

# Project Description
Sparkify is an imaginary company that had their use base increase
 and would like to introduce automation and monitoring to their ETL pipelines.
 
The data pipeline need to have the following attributes:
- Dynamic and built from reusable tasks
- Allows backfills
- Runs data quality checks

The data pipeline should stage data into Sparkify cloud data warehouse Redshift.
It should process the data afterward loading into a star schema with the appropriate dimension and fact tables. 

# Used Data sets
The source data resides in S3 buckets on amazon web services. It consists of the following data sets:
- song data set
- log data set

## Song Data Set
Subset of the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/). Each file is in JSON format.
Each file contains information about song e.g., artist_name, song_id, etc.
Files are partitioned by the first three letters of each song's track ID.

## Log Data Set
Consists of log files generated by [this](https://github.com/Interana/eventsim) event simulator. 
Although this dataset shouldn't be used as a basis to understand user behavior it gives great insight on 
what user activity on a music streaming service should look like. 

# Project Hierarchy
The project contains the following:
- dags
- plugins
- create_tables.sql
Folder contents should be moved under `AIRFLOW_HOME` directory.

## dags
Contains the defined pipeline as a DAG (Directed Acyclic Graph).
It contains a single file `udac_example_dag.py`

## plugins
Contains all user-defined operators, as well as helper class containing useful SQL statements. 
It contains the following operators:

- **StageToRedshiftOperator**: stage data from S3 buckets into staging area
- **LoadFactOperator**: process staging tables and load results into fact table
- **LoadDimensionOperator**: process staging tables and load results into dimensional tables
- **DataQualityOperator**: run various data quality checks on the pipeline

## create_tables.sql
Contains useful SQL queries to create dimensional model tables.

# Project Run
To run the project please [install airflow](https://airflow.apache.org/docs/stable/installation.html)
and move the content of **dags** and **plugins** folders under `AIRFLOW_HOME` **dags** and **plugins** folders
respectively.
Run airflow server [running airflow link](https://airflow.apache.org/docs/stable/start.html)

